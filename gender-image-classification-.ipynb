{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":844929,"sourceType":"datasetVersion","datasetId":446365}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-08T05:21:23.447922Z","iopub.execute_input":"2024-10-08T05:21:23.448317Z","iopub.status.idle":"2024-10-08T05:21:23.921077Z","shell.execute_reply.started":"2024-10-08T05:21:23.448279Z","shell.execute_reply":"2024-10-08T05:21:23.919912Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport random\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:21:23.923259Z","iopub.execute_input":"2024-10-08T05:21:23.924121Z","iopub.status.idle":"2024-10-08T05:21:24.188960Z","shell.execute_reply.started":"2024-10-08T05:21:23.924088Z","shell.execute_reply":"2024-10-08T05:21:24.187675Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def convolution(image, kernel):\n    image = image[0]\n    if kernel.ndim ==4:\n        kernel = kernel[0,0]\n    else:\n        kernel = kernel[0]\n    image_height, image_width = image.shape\n    kernel_height, kernel_width = kernel.shape\n\n    result_height = image_height - kernel_height + 1\n    result_width = image_width - kernel_width + 1\n\n    result = np.zeros((result_height, result_width))\n\n    for i in range(result_height):\n        for j in range(result_width):\n            result[i, j] = np.sum(image[i:i+kernel_height, j:j+kernel_width] * kernel)\n\n    return result\n\ndef same_convolution(image, kernel):\n    padded_image = np.pad(image, ((0, kernel.shape[0] - 1), (0, kernel.shape[1] - 1)), mode='constant')\n    return convolution(padded_image, kernel)\n\ndef full_convolution(image, kernel):\n    padded_image = np.pad(image, 1, mode='constant')\n    padded_image = np.pad(padded_image, 1, mode='constant')\n    flipped_kernel = flip_matrix_180(kernel)\n    return np.array([convolution(padded_image, flipped_kernel)]) \n\ndef flip_matrix_180(matrix):\n    return np.rot90(matrix, 2)\n\ndef cross_correlation(matrix1, matrix2):\n    return np.sum(np.matmul(matrix1, matrix2))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:21:24.190787Z","iopub.execute_input":"2024-10-08T05:21:24.191180Z","iopub.status.idle":"2024-10-08T05:21:24.207350Z","shell.execute_reply.started":"2024-10-08T05:21:24.191145Z","shell.execute_reply":"2024-10-08T05:21:24.205914Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def mse(predicted, actual):\n    return np.mean((predicted - actual)**2)\ndef mse_prime( predicted,actual):\n    return 2*(predicted-actual)/actual.size","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:21:24.210583Z","iopub.execute_input":"2024-10-08T05:21:24.211026Z","iopub.status.idle":"2024-10-08T05:21:24.218299Z","shell.execute_reply.started":"2024-10-08T05:21:24.210988Z","shell.execute_reply":"2024-10-08T05:21:24.217070Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def logloss(y_true, y_pred, eps=1e-15):\n    y_pred = np.clip(y_pred, eps, 1 - eps)\n    return -(y_true * np.log(y_pred)).sum(axis=1).mean()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:21:24.219886Z","iopub.execute_input":"2024-10-08T05:21:24.220921Z","iopub.status.idle":"2024-10-08T05:21:24.236908Z","shell.execute_reply.started":"2024-10-08T05:21:24.220881Z","shell.execute_reply":"2024-10-08T05:21:24.235864Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def logloss_prime(y_true, y_pred):\n    return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:21:24.238722Z","iopub.execute_input":"2024-10-08T05:21:24.239495Z","iopub.status.idle":"2024-10-08T05:21:24.248361Z","shell.execute_reply.started":"2024-10-08T05:21:24.239453Z","shell.execute_reply":"2024-10-08T05:21:24.247228Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class Layer:\n    def __init__(self):\n        self.inp = None\n        self.output = None\n    def forward(self,inp):\n        pass\n    def backward(self,learning_rate,output_grad):\n        pass","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:21:24.250151Z","iopub.execute_input":"2024-10-08T05:21:24.250912Z","iopub.status.idle":"2024-10-08T05:21:24.260782Z","shell.execute_reply.started":"2024-10-08T05:21:24.250871Z","shell.execute_reply":"2024-10-08T05:21:24.259557Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Dense_layer(Layer):\n    def __init__(self,inp_size,output_size):\n        self.weights = np.random.randn(output_size,inp_size)\n        self.bias = np.random.randn(output_size,1) \n    def forward(self,inp):\n        self.inp_d = inp\n        return np.dot(self.weights,self.inp_d) + self.bias\n    def backward(self,learning_rate,output_grad):\n        weights_grad = np.dot(output_grad,(self.inp_d).T) \n        inp_grad = np.dot((self.weights).T,output_grad)\n        self.weights -= learning_rate * weights_grad\n        self.bias -= learning_rate * output_grad\n        return inp_grad","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:21:24.262466Z","iopub.execute_input":"2024-10-08T05:21:24.263198Z","iopub.status.idle":"2024-10-08T05:21:24.272883Z","shell.execute_reply.started":"2024-10-08T05:21:24.263159Z","shell.execute_reply":"2024-10-08T05:21:24.271849Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class activation_layer(Layer):\n    def __init__(self,activation,activation_prime):\n        self.activation = activation\n        self.activation_prime = activation_prime\n    def forward(self,inp):\n        self.inp_a = inp\n        return self.activation(self.inp_a)\n    def backward(self,learning_rate,output_grad):\n        return np.multiply(output_grad,self.activation(self.inp_a))","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:21:24.274585Z","iopub.execute_input":"2024-10-08T05:21:24.275340Z","iopub.status.idle":"2024-10-08T05:21:24.284468Z","shell.execute_reply.started":"2024-10-08T05:21:24.275300Z","shell.execute_reply":"2024-10-08T05:21:24.283305Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class ReLU(activation_layer):\n    def __init__(self):\n        def ReLU(x):\n            return x * (x > 0)\n\n        def dReLU(x):\n            return 1. * (x > 0)\n        \n        super().__init__(ReLU, dReLU)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:21:24.290504Z","iopub.execute_input":"2024-10-08T05:21:24.291033Z","iopub.status.idle":"2024-10-08T05:21:24.300548Z","shell.execute_reply.started":"2024-10-08T05:21:24.290988Z","shell.execute_reply":"2024-10-08T05:21:24.299388Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Sigmoid(activation_layer):\n    def __init__(self):\n        def sigmoid(x):\n            x = np.float128(x) \n            return 1 / (1 + np.exp(-x))\n\n        def sigmoid_prime(x):\n            s = sigmoid(x)\n            return s * (1 - s)\n\n        super().__init__(sigmoid, sigmoid_prime)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:21:24.302322Z","iopub.execute_input":"2024-10-08T05:21:24.303088Z","iopub.status.idle":"2024-10-08T05:21:24.312658Z","shell.execute_reply.started":"2024-10-08T05:21:24.303049Z","shell.execute_reply":"2024-10-08T05:21:24.311687Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class Convolutional(Layer):\n    def __init__(self, inp_shape, kernel_size, depth):\n        inp_depth, inp_height, inp_width = inp_shape\n        self.depth = depth\n        self.inp_shape = inp_shape\n        self.inp_depth = inp_depth\n        self.output_shape = (depth, inp_height - kernel_size + 1, inp_width - kernel_size + 1)\n        self.kernels_shape = ( depth, inp_depth,kernel_size, kernel_size)\n        self.kernels = np.random.randn(*self.kernels_shape)\n        self.biases = np.random.randn(*self.output_shape)\n\n    def forward(self, inp):\n        self.inp_c = inp\n        self.output = np.copy(self.biases)\n        for i in range(self.depth):\n            for j in range(self.inp_depth):\n                self.output += convolution(self.inp_c, self.kernels)\n        return self.output\n\n    def backward(self, learning_rate, output_grad):\n        kernels_grad = np.zeros(self.kernels_shape)\n        inp_grad = np.zeros(self.inp_shape)\n\n        for i in range(self.depth):\n            for j in range(self.inp_depth):\n                kernels_grad = convolution(self.inp_c, output_grad)\n                inp_grad += full_convolution(output_grad, self.kernels)\n\n        self.kernels -= learning_rate * kernels_grad\n        self.biases -= learning_rate * output_grad\n        return inp_grad","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:21:24.314400Z","iopub.execute_input":"2024-10-08T05:21:24.315206Z","iopub.status.idle":"2024-10-08T05:21:24.330140Z","shell.execute_reply.started":"2024-10-08T05:21:24.315115Z","shell.execute_reply":"2024-10-08T05:21:24.329110Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class Change_shape(Layer):\n    def __init__(self, inp_shape, output_shape):\n        self.inp_shape = inp_shape\n        self.output_shape = output_shape\n\n    def forward(self, inp):\n        return np.reshape(inp, self.output_shape)\n\n    def backward(self,learning_rate, output_grad):\n        return np.reshape(output_grad, self.inp_shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:21:24.331971Z","iopub.execute_input":"2024-10-08T05:21:24.332797Z","iopub.status.idle":"2024-10-08T05:21:24.342247Z","shell.execute_reply.started":"2024-10-08T05:21:24.332756Z","shell.execute_reply":"2024-10-08T05:21:24.341287Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Max_pool(Layer):\n    def __init__(self, kernel_size, stride):\n        self.kernel_size = kernel_size\n        self.stride = stride\n        \n    def forward(self, inp):\n        \n        result_height = inp.shape[1] // self.kernel_size\n        result_width = inp.shape[2] // self.kernel_size\n\n        result = np.zeros((result_height, result_width))\n\n        for i in range(result_height):\n            for j in range(result_width):\n                result[i, j] = np.max(inp[i*self.kernel_size:(i+1)*self.kernel_size, j*self.kernel_size:(j+1)*self.kernel_size])\n        print(result)\n        return result\n\n    def backward(self,learning_rate, output_grad):\n        \n        height, width = output_grad.shape\n        result_height = height * self.kernel_size\n        result_width = width * self.kernel_size\n\n        result = np.zeros((result_height, result_width))\n\n        for i in range(height):\n            for j in range(width):\n                max_value = output_grad[i, j]\n                max_row = i * self.kernel_size\n                max_col = j * self.kernel_size\n                result[max_row:max_row + self.kernel_size, max_col:max_col + self.kernel_size] = 0\n                result[max_row + np.argmax(max_value) // self.kernel_size, max_col + np.argmax(max_value) % self.kernel_size] = max_value\n\n        return result","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:21:24.344029Z","iopub.execute_input":"2024-10-08T05:21:24.344778Z","iopub.status.idle":"2024-10-08T05:21:24.360386Z","shell.execute_reply.started":"2024-10-08T05:21:24.344738Z","shell.execute_reply":"2024-10-08T05:21:24.359346Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def load_data(class_0_folder, class_1_folder):\n    class_0_images = []\n    for filename in os.listdir(class_0_folder):\n        if filename.endswith(\".jpg\") or filename.endswith(\".jpg.jpg\"):\n            image_path = os.path.join(class_0_folder, filename)\n            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n            img = cv2.resize(img, (80, 80))  # Resize the image to a fixed size \n            \n            class_0_images.append((np.array([img]), [[0],[1]]))\n\n    class_1_images = []\n    for filename in os.listdir(class_1_folder):\n        if filename.endswith(\".jpg\") or filename.endswith(\".jpg.jpg\"):\n            image_path = os.path.join(class_1_folder, filename)\n            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n            img = cv2.resize(img, (80, 80))  # Resize the image to a fixed size \n            \n            class_1_images.append((np.array([img]),[[1],[0]]))\n\n    return class_0_images + class_1_images\n\ndef preprocess_data(data):\n    processed_data = [[image / 255.0, label] for image, label in data]  # Normalize pixel values to the range [0, 1]\n \n    return processed_data\n    \ndef load_validation_test_data(test_class_0_folder, test_class_1_folder):\n    \n\n    test_data = []\n    for class_0_filename in os.listdir(test_class_0_folder):\n        if class_0_filename.endswith(\".jpg.jpg\") or class_0_filename.endswith(\".jpg\"):\n            image_path = os.path.join(test_class_0_folder, class_0_filename)\n            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n            img = cv2.resize(img, (80, 80))  # Resize the image to a fixed size\n            \n            test_data.append((np.array([img]), [[0],[1]]))  # Label 0 for class 0\n    \n    for class_1_filename in os.listdir(test_class_1_folder):\n        if class_1_filename.endswith(\".jpg\") or class_1_filename.endswith(\".jpg.jpg\"):\n            image_path = os.path.join(test_class_1_folder, class_1_filename)\n            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n            img = cv2.resize(img, (80, 80))  # Resize the image to a fixed size \n            \n            test_data.append((np.array([img]), [[1],[0]]))  # Assign a placeholder label, adjust as needed\n\n    return test_data\n\ndef train_cnn(model, loss, loss_prime, xtrain, ytrain, epochs, learning_rate, batch_size):\n    \"\"\"\n    Train the CNN model using a simple training loop.\n    \"\"\"\n    n_samples = len(xtrain)  # Number of samples in the training set\n    for e in range(epochs):\n        error = 0\n        start = 0\n        while start < n_samples:\n            # Define end for the current batch\n            end = start + batch_size\n            x_train = xtrain[start:end]\n            y_train = ytrain[start:end]\n\n            # Iterate over each sample in the batch\n            for x, y in zip(x_train, y_train):\n                # Forward pass\n                output = predict(model, x)\n\n                # Calculate loss/error\n                error += loss(y, output)\n\n                # Backward pass\n                grad = loss_prime(y, output)\n                for layer in reversed(model):\n                    grad = layer.backward(learning_rate, grad)\n\n            # Move to the next batch\n            start += batch_size\n\n        # Compute average error for this epoch\n        error /= len(xtrain)\n        print(f\"Epoch {e + 1}/{epochs}, Loss={error}\")\n\n        \n            \ndef predict(network, inp):\n    output = inp\n    for layer in network:\n        output = layer.forward(output)\n    return output\n\n\ndef test_cnn(model, test_images, labels):\n    \"\"\"\n    Test the CNN model on a set of images.\n    \"\"\"\n    predictions = []\n    for image in test_images:\n        # Forward pass for each test image\n        output = model.forward(image)\n        predictions.append(output)\n\n    # Compute the mean squared error between predictions and actual labels\n    prediction_error = mean_squared_error(predictions, labels)\n    \n    return prediction_error\n\n\nif __name__ == \"__main__\":\n    class_0_folder = '/kaggle/input/gender-classification-dataset/Training/female'\n    class_1_folder = '/kaggle/input/gender-classification-dataset/Training/male'\n    test_class_0_folder = '/kaggle/input/gender-classification-dataset/Validation/female'\n    test_class_1_folder = '/kaggle/input/gender-classification-dataset/Validation/male'\n\n    # Load and preprocess training data\n    training_data = load_data(class_0_folder, class_1_folder) \n    random.shuffle(training_data)\n    \n    processed_training_data = preprocess_data(training_data)\n    \n\n    # Separate images and labels for training\n    train_images, train_labels = zip(*processed_training_data)\n    train_images = np.array(train_images)\n    train_labels = np.array(train_labels)\n\n    # Create and train the CNN model\n    network = [Convolutional((1, 80, 80), 3, 1),Sigmoid(),Change_shape((1, 78, 78), (6084, 1)),Dense_layer(6084, 250),Sigmoid(),Dense_layer(250, 40),Sigmoid(),Dense_layer(40, 2),Sigmoid()]\n    train_cnn(network, logloss, logloss_prime, train_images, train_labels, epochs=10, learning_rate=0.00001,batch_size=500)\n\n    # Load test data\n    test_data = load_validation_test_data(test_class_0_folder, test_class_1_folder\n    )\n\n    random.shuffle(test_data)\n    \n    processed_test_data = preprocess_data(test_data)\n    test_images, test_labels = zip(*processed_test_data)\n    test_images = np.array(test_images)\n    test_labels = np.array(test_labels)\n    \n    predictions = []\n    Y = []\n    itet = 0\n    for x, y in zip(test_images, test_labels):\n        output = predict(network, x)\n        predictions.append(np.argmax(output))\n        Y.append(np.argmax(y))\n        itet +=1\n        \n    def get_accuracy(predictions, Y):\n        return np.sum(predictions == Y) / Y.size\n    predictions = np.array(predictions)\n    Y = np.array(Y)\n        \n    print(\"Accuracy: \",get_accuracy(predictions,Y))","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:21:24.362427Z","iopub.execute_input":"2024-10-08T05:21:24.363242Z","iopub.status.idle":"2024-10-08T05:32:04.367530Z","shell.execute_reply.started":"2024-10-08T05:21:24.363200Z","shell.execute_reply":"2024-10-08T05:32:04.366477Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/10, Loss=0.371149320840045\nEpoch 2/10, Loss=0.3540602113976791\nEpoch 3/10, Loss=0.3457493261357331\nEpoch 4/10, Loss=0.3609704686108311\nEpoch 5/10, Loss=0.3426194442408264\nEpoch 6/10, Loss=0.1716366606760628\nEpoch 7/10, Loss=0.143090854218041\nEpoch 8/10, Loss=0.1798980652981318\nEpoch 9/10, Loss=0.0055358963966433\nEpoch 10/10, Loss=0.0052096236703542\nAccuracy:  0.93\n","output_type":"stream"}]},{"cell_type":"code","source":"class Logistic_Regression():\n    \n\n    def __init__(self, alpha, num_iter):\n\n        self.alpha = alpha\n        self.num_iter = num_iter\n\n    def fit(self, X, Y):\n        self.m, self.n,self.l = X.shape\n        self.w = np.zeros((self.n,2))\n        self.b = np.zeros((2,1))\n        self.X = X\n        self.Y = Y\n\n        for i in range(self.num_iter):\n            self.update_weights()\n\n    def update_weights(self):\n        for j in range(len(self.X)):\n            Y_hat = 1 / (1 + np.exp( - (self.w.T.dot(self.X[j]) + self.b ) ))    \n            dw = (1/self.m)*np.dot(self.X[j], (Y_hat - self.Y[j]).T)\n            db = (1/self.m)*np.sum(Y_hat - self.Y)\n\n\n            self.w = self.w - self.alpha * dw\n            self.b = self.b - self.alpha * db\n\n\n    def predict(self, X):\n        itet = 0\n        predictions =[]\n        for i in range(len(X)):\n            Y_pred = 1 / (1 + np.exp( - (self.w.T.dot(X[i]) + self.b ) )) \n            Y_pred = np.where( Y_pred > 0.5, 1, 0)\n            predictions.append(Y_pred)\n            itet +=1\n        return np.array(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:32:04.368937Z","iopub.execute_input":"2024-10-08T05:32:04.369265Z","iopub.status.idle":"2024-10-08T05:32:04.381874Z","shell.execute_reply.started":"2024-10-08T05:32:04.369235Z","shell.execute_reply":"2024-10-08T05:32:04.380764Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"lr = Logistic_Regression(0.1,15)\ntrain = []\nfor i in train_images:\n    i = np.reshape(i,(6400,1))                         \n    train.append(i)\ntrain = np.array(train) \nlr.fit(train,train_labels)\ntest = []    \n    \nrandom.shuffle(processed_test_data)\n\ntest_images, test_labels = zip(*processed_test_data)\ntest_images = np.array(test_images)\ntest_labels = np.array(test_labels)    \n\nfor i in test_images:\n    i = np.reshape(i,(6400,1))                         \n    test.append(i)\n    \npredictions = lr.predict(test)\ndef get_accuracy(predictions, Y):\n    return np.sum(predictions == Y) / Y.size\n    \nprint(\"Accuracy: \",get_accuracy(predictions,test_labels))","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:32:04.383371Z","iopub.execute_input":"2024-10-08T05:32:04.383907Z","iopub.status.idle":"2024-10-08T05:32:04.734726Z","shell.execute_reply.started":"2024-10-08T05:32:04.383867Z","shell.execute_reply":"2024-10-08T05:32:04.733197Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Accuracy:  0.76\n","output_type":"stream"}]},{"cell_type":"code","source":"#Label encoding\ndef label_encode(Y):\n    y_label = []\n    for i in Y:\n        if (i == np.array([[0],[1]])).all():\n            y_label.append(-1)\n        else:\n            y_label.append(1)\n    return np.array(y_label)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:32:04.736761Z","iopub.execute_input":"2024-10-08T05:32:04.737642Z","iopub.status.idle":"2024-10-08T05:32:04.748470Z","shell.execute_reply.started":"2024-10-08T05:32:04.737564Z","shell.execute_reply":"2024-10-08T05:32:04.746555Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class SVM_classifier():\n\n\n    def __init__(self, learning_rate, no_of_iterations, lambda_parameter):\n\n        self.learning_rate = learning_rate\n        self.no_of_iterations = no_of_iterations\n        self.lambda_parameter = lambda_parameter\n\n\n    def fit(self, X, Y):\n        self.m, self.n,self.l  = X.shape\n        self.w = np.zeros((self.n,1))\n        self.b = 0\n        self.X = X\n        self.Y = Y\n\n        # implementing Gradient Descent algorithm for Optimization\n\n        for i in range(self.no_of_iterations):\n            self.update_weights()\n\n\n    def update_weights(self):\n\n        y_label = label_encode(self.Y)\n        for index, x_i in enumerate(self.X):\n            \n            condition = y_label[index] * (np.dot(self.w.T,x_i) - self.b) >= 1\n            if (condition == True).all():\n\n                dw = 2 * self.lambda_parameter * self.w\n                db = 0\n\n            else:\n\n                dw = 2 * self.lambda_parameter * self.w - np.dot(x_i, y_label[index])\n                db = y_label[index]\n\n\n            self.w = self.w - self.learning_rate * dw\n            self.b = self.b - self.learning_rate * db\n\n\n\n      # predict the label for a given input value\n    def predict(self, X):\n        itet = 0\n        predictions = []\n        for i in range(len(X)):\n            output = np.dot(self.w.T,X[i]) - self.b \n            predicted_labels = np.sign(output)\n            #y_hat = np.where(predicted_labels <= -1, -1, 1)\n            if predicted_labels <=-1:\n                y_hat = -1\n            else:\n                y_hat = 1\n            predictions.append(y_hat)\n            itet +=1\n\n        return np.array(predictions)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:32:04.751248Z","iopub.execute_input":"2024-10-08T05:32:04.752298Z","iopub.status.idle":"2024-10-08T05:32:04.780309Z","shell.execute_reply.started":"2024-10-08T05:32:04.752238Z","shell.execute_reply":"2024-10-08T05:32:04.778559Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"svm = SVM_classifier(0.00001,15,1)\ntrain = []\nfor i in train_images:\n    i = np.reshape(i,(6400,1))                         \n    train.append(i)\ntrain = np.array(train)\nsvm.fit(train,train_labels)\ntest = []\ntest_data = load_validation_test_data(test_class_0_folder, test_class_1_folder)\nrandom.shuffle(test_data)\n\nprocessed_test_data = preprocess_data(test_data)\ntest_images, test_labels = zip(*processed_test_data)\ntest_images = np.array(test_images)\ntest_labels = np.array(test_labels)\nfor i in test_images:\n    i = np.reshape(i,(6400,1))                         \n    test.append(i)\ntest = np.array(test)\npredictions = svm.predict(test)\ntest_labels = label_encode(test_labels)\ndef get_accuracy(predictions, Y):\n    return np.sum(predictions == Y) / Y.size\n    \nprint(\"Accuracy:\",get_accuracy(predictions,test_labels))","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:32:04.783071Z","iopub.execute_input":"2024-10-08T05:32:04.784344Z","iopub.status.idle":"2024-10-08T05:32:19.387791Z","shell.execute_reply.started":"2024-10-08T05:32:04.784052Z","shell.execute_reply":"2024-10-08T05:32:19.386700Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Accuracy:  0.77\n","output_type":"stream"}]}]}